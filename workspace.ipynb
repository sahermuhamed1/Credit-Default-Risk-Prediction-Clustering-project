{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "363aab0f",
   "metadata": {},
   "source": [
    "## üìö Table of Contents\n",
    "\n",
    "### 1Ô∏è‚É£ Data Loading & Cleaning\n",
    "- [1.1 üì• Load Dataset](#load-dataset)\n",
    "- [2.2 üßπ Data Cleaning](#data-cleaning)\n",
    "\n",
    "### 2Ô∏è‚É£ Distribution Analysis\n",
    "- [2.1 üìä Distribution of Key Variables](#distribution-variables)\n",
    "- [2.2 ‚öñÔ∏è Target Imbalance Check](#target-imbalance)\n",
    "\n",
    "### 3Ô∏è‚É£ Outlier Removal\n",
    "- [3.1 üìê Z-score Method](#zscore)\n",
    "- [3.2 üì¶ IQR Method](#iqr)\n",
    "\n",
    "### 4Ô∏è‚É£ Feature Engineering\n",
    "- [4.1 üõ†Ô∏è Financial Ratios (Utilization, Payment to Depet Ratio)](#financial-features)\n",
    "\n",
    "### 5Ô∏è‚É£ RFM Analysis\n",
    "- [5.1 ‚è±Ô∏è Recency](#recency)\n",
    "- [5.2 üîÅ Frequency](#frequency)\n",
    "- [5.3 üí∞ Monetary](#monetary)\n",
    "\n",
    "### 6Ô∏è‚É£ EDA\n",
    "- [6.1 üìä EDA](#eda)\n",
    "\n",
    "### 7Ô∏è‚É£ Statistical Inference\n",
    "- [7.1 üìè Confidence Intervals](#confidence-interval)\n",
    "- [7.2 üìä t-test](#ttest)\n",
    "- [7.3 üßÆ Chi-Square Test](#chi-square)\n",
    "\n",
    "### 8Ô∏è‚É£ Imbalance Handling\n",
    "- [8.1 ‚öñÔ∏è SMOTE Oversampling](#smote)\n",
    "\n",
    "### 9Ô∏è‚É£ ML Modeling\n",
    "- [9.1 ü§ñ Logistic Regression & ROC AUC](#logistic-baseline)\n",
    "- [9.2 üìä Tree and Gradient Boosting Models](#tree-boosting)\n",
    "- [9.3 üéØ Best Model](#best-model)\n",
    "\n",
    "### üîü Statistical Sampling\n",
    "- [10.1 üîÑ Bootstrap Sampling](#bootstrap)\n",
    "- [10.2 üîÄ Permutation Testing](#permutation)\n",
    "\n",
    "### 11 Clustering & Dimensionality Reduction\n",
    "- [11.1 üåÄ KMeans & DBSCAN](#clustering)\n",
    "- [11.2 üìâ PCA Transformation](#pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e16e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7f3077",
   "metadata": {},
   "source": [
    "## 1.1 üì• Load Dataset <a id=\"load-dataset\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99206ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/UCI_Credit_Card.csv')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61786c24",
   "metadata": {},
   "source": [
    "### There are 25 variables:\n",
    "\n",
    "- ID: ID of each client\n",
    "- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit)\n",
    "- SEX: Gender (1=male, 2=female)\n",
    "- EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
    "- MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
    "- AGE: Age in years\n",
    "- PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, ... 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
    "- PAY_2: Repayment status in August, 2005 (scale same as above)\n",
    "- PAY_3: Repayment status in July, 2005 (scale same as above)\n",
    "- PAY_4: Repayment status in June, 2005 (scale same as above)\n",
    "- PAY_5: Repayment status in May, 2005 (scale same as above)\n",
    "- PAY_6: Repayment status in April, 2005 (scale same as above)\n",
    "- BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
    "- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
    "- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
    "- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
    "- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
    "- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
    "- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
    "- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
    "- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
    "- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
    "- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
    "- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
    "- default.payment.next.month: Default payment (1=yes, 0=no)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525f937",
   "metadata": {},
   "source": [
    "## 1.2 üßπ Data Cleaning <a id=\"data-cleaning\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892f7cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d09066",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df.rename(columns={'pay_0':'status_sep',\n",
    "                    'pay_2': 'status_aug',\n",
    "                    'pay_3': 'status_jul',\n",
    "                    'pay_4': 'status_june',\n",
    "                    'pay_5': 'status_may',\n",
    "                    'pay_6': 'status_apr',\n",
    "                    'bill_amt1': 'debt_sep',\n",
    "                    'bill_amt2': 'debt_aug',\n",
    "                    'bill_amt3': 'debt_jul',\n",
    "                    'bill_amt4': 'debt_june',\n",
    "                    'bill_amt5': 'debt_may',\n",
    "                    'bill_amt6': 'debt_apr',\n",
    "                    'pay_amt1': 'pay_sep',\n",
    "                    'pay_amt2': 'pay_aug',\n",
    "                    'pay_amt3': 'pay_jul',\n",
    "                    'pay_amt4': 'pay_june',\n",
    "                    'pay_amt5': 'pay_may',\n",
    "                    'pay_amt6': 'pay_apr',\n",
    "                    'default.payment.next.month': 'default_payment'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686499e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67955e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa3ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cols = [\n",
    "    \"id\",\"limit_bal\",\"sex\",\"education\",\"marriage\",\"age\"\n",
    "]\n",
    "pay_cols = ['status_sep','status_aug', 'status_jul', 'status_june', 'status_may', 'status_apr']\n",
    "bill_cols = ['debt_sep','debt_aug', 'debt_jul', 'debt_june', 'debt_may', 'debt_apr']\n",
    "pay_amt_cols = ['pay_sep','pay_aug', 'pay_jul', 'pay_june', 'pay_may', 'pay_apr']\n",
    "target_col = [\"default_payment\"]\n",
    "\n",
    "df = df[base_cols + pay_cols + bill_cols + pay_amt_cols + target_col]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae122c",
   "metadata": {},
   "source": [
    "## 2.1 üìä Distribution of Key Variables <a id=\"distribution-variables\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeae0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481c08a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(20, 15), bins=30)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9c80d",
   "metadata": {},
   "source": [
    "## 2.2 ‚öñÔ∏è Target Imbalance Check <a id=\"target-imbalance\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0709a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['default_payment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609366d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['default_payment'].value_counts().plot.pie(\n",
    "    autopct='%1.1f%%', \n",
    "    figsize=(4, 4), \n",
    "    startangle=90\n",
    ")\n",
    "plt.ylabel(\"\")\n",
    "plt.title(\"Default Payment Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615428c5",
   "metadata": {},
   "source": [
    "#### **the target column is inbalanced so we need to oversample it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f15e9",
   "metadata": {},
   "source": [
    "## 3.1 üìê Z-score Method to remove the outliers <a id=\"zscore\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84347a5c",
   "metadata": {},
   "source": [
    "### use z-score when the data is normally distributed and IQR when it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1108c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "# Example: apply on numeric columns only\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Calculate Z-scores\n",
    "z_scores = df[numeric_cols].apply(zscore)\n",
    "\n",
    "# Keep rows where all z-scores are within threshold (e.g., ¬±3)\n",
    "df_no_outliers = df[(np.abs(z_scores) < 3).all(axis=1)]\n",
    "\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"After removing outliers: {df_no_outliers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819d43cd",
   "metadata": {},
   "source": [
    "## 3.2 üì¶ IQR Method to remove the outliers <a id=\"iqr\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282c338e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df):\n",
    "    \"\"\"\n",
    "    Remove outliers from all numeric columns using IQR.\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc1627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_outliers = remove_outliers_iqr(df)\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"After removing outliers: {df_no_outliers.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4e1618",
   "metadata": {},
   "source": [
    "### from this we can conclude that the data contains a lot of outliers, a z-score removed 4,000 rows and IQR removes 22,000 rows because it is more sensitive to the distribution of the data.\n",
    "\n",
    "## **since the data we use is a banking data and it's ok to have outliers, so we will consider the data without removing outliers and will continue with the original df.**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa79bfaf",
   "metadata": {},
   "source": [
    "## 4.1 üõ†Ô∏è Financial Ratios (Utilization, Avg Payment) <a id=\"financial-features\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef570af",
   "metadata": {},
   "source": [
    "Utilization: Shows how much of the credit limit is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in ['apr','may','june','jul','aug','sep']:\n",
    "    df[f'utilization_{month}'] = df[f'debt_{month}'] / df['limit_bal']\n",
    "df['avg_utilization'] = df[[f'utilization_{m}' for m in ['apr','may','june','jul','aug','sep']]].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b10a72",
   "metadata": {},
   "source": [
    "Payment-to-Debt Ratio (per month + overall): Measures whether payments are covering outstanding debt. [less than 1, then the payments doesn't cover all the dept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e0d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for month in ['apr','may','june','jul','aug','sep']:\n",
    "    df[f'pay_ratio_{month}'] = df[f'pay_{month}'] / (df[f'debt_{month}'] + 1e-6)  # avoid divide by 0\n",
    "df['avg_pay_ratio'] = df[[f'pay_ratio_{m}' for m in ['apr','may','june','jul','aug','sep']]].mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f785f8",
   "metadata": {},
   "source": [
    "pay_ratio_std: Checks stability of payments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d637c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pay_ratio_std'] = df[[f'pay_ratio_{m}' for m in ['apr','may','june','jul','aug','sep']]].std(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92fdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_delay'] = df[[f'status_{m}' for m in ['apr','may','june','jul','aug','sep']]].mean(axis=1)\n",
    "df['max_delay'] = df[[f'status_{m}' for m in ['apr','may','june','jul','aug','sep']]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb144627",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = df['default_payment']\n",
    "df.drop('default_payment', axis=1, inplace=True)\n",
    "df['default_payment'] = dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22e6cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab4c9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_d = df['avg_delay']\n",
    "max_d = df['max_delay']\n",
    "df.drop(['avg_delay', 'max_delay'], axis=1, inplace=True)\n",
    "\n",
    "# Find position after 'status_apr'\n",
    "idx = df.columns.get_loc('status_apr') + 1\n",
    "\n",
    "df = pd.concat([df.iloc[:, :idx], avg_d, max_d, df.iloc[:, idx:]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eecf9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fd2a16",
   "metadata": {},
   "source": [
    "## 5.1 ‚è±Ô∏è Recency <a id=\"recency\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b8fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['apr','may','june','jul','aug','sep']\n",
    "month_order = {m: i+1 for i, m in enumerate(months)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781b8adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recency ‚Üí latest month with nonzero payment\n",
    "df['R'] = df[[f'pay_{m}' for m in months]].apply(\n",
    "    lambda row: max([month_order[m] for m in months if row[f'pay_{m}'] > 0], default=0),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df['R'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bab345",
   "metadata": {},
   "source": [
    "## 5.2 üîÅ Frequency <a id=\"frequency\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa7815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency ‚Üí number of trxn made in the period (Apr‚ÄìSep) with nonzero payment\n",
    "df['F'] = df[[f'pay_{m}' for m in months]].gt(0).sum(axis=1)\n",
    "\n",
    "df['F'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd36caa",
   "metadata": {},
   "source": [
    "## 5.3 üí∞ Monetary <a id=\"monetary\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf63abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monetary ‚Üí total payment across Apr‚ÄìSep\n",
    "df['M'] = df[[f'pay_{m}' for m in months]].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9271987",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386f2df6",
   "metadata": {},
   "source": [
    "## 6.1 üìä EDA <a id=\"eda\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2340bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "num_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# calculate grid size\n",
    "n_cols = 4\n",
    "n_rows = math.ceil(len(num_cols) / n_cols)\n",
    "\n",
    "plt.figure(figsize=(5*n_cols, 4*n_rows))\n",
    "\n",
    "for i, column in enumerate(num_cols, 1):\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    sns.boxplot(x=df[column])\n",
    "    plt.title(column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a28a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "\n",
    "# mask for upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Lower-Triangle Correlation Heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e164e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of limit_bil\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(df['limit_bal'], bins=30, kde=True)\n",
    "plt.title(\"Distribution of Credit Limit\")\n",
    "plt.xlabel(\"Credit Limit\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3202f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relation between Relationship between limit_bal & default rate using line plot\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.lineplot(x='limit_bal', y='default_payment', data=df)\n",
    "plt.title(\"Relationship between Credit Limit and Default Rate\")\n",
    "plt.xlabel(\"Credit Limit\")\n",
    "plt.ylabel(\"Default Rate\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b90e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of repayment status (status_apr ‚Üí status_sep).\n",
    "repayment_status = df.filter(like='status_')\n",
    "plt.figure(figsize=(12, 8))\n",
    "mask = np.triu(np.ones_like(repayment_status.corr(), dtype=bool))\n",
    "sns.heatmap(repayment_status.corr(), mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Heatmap of Repayment Status\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend lines: average debt and payment over months\n",
    "debt_cols = ['debt_apr', 'debt_may', 'debt_june', 'debt_jul', 'debt_aug', 'debt_sep']\n",
    "pay_cols  = ['pay_apr',  'pay_may',  'pay_june',  'pay_jul',  'pay_aug',  'pay_sep']\n",
    "\n",
    "# Calculate averages for each month\n",
    "avg_debt = df[debt_cols].mean()\n",
    "avg_pay  = df[pay_cols].mean()\n",
    "\n",
    "# Create dataframe for plotting\n",
    "trend_df = pd.DataFrame({\n",
    "    \"Month\": [\"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\"],\n",
    "    \"Avg Debt\": avg_debt.values,\n",
    "    \"Avg Payment\": avg_pay.values\n",
    "})\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(trend_df[\"Month\"], trend_df[\"Avg Debt\"], marker='o', label=\"Average Debt\")\n",
    "plt.plot(trend_df[\"Month\"], trend_df[\"Avg Payment\"], marker='o', label=\"Average Payment\")\n",
    "plt.title(\"Trend of Average Debt and Payment (Apr - Sep)\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Amount\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e5774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target correlation plot (which features are most predictive of default_payment).\n",
    "target_corr = df.corr()['default_payment'].sort_values(ascending=False)\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.barplot(x=target_corr.index, y=target_corr.values)\n",
    "plt.title(\"Feature Correlation with Default Payment\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d818a8b",
   "metadata": {},
   "source": [
    "## 7.1 üìè Confidence Intervals<a id=\"confidence-interval\"></a>\n",
    "### get the confidence interval of a mean (or any statistic) that the population is likely to fall into\n",
    "CI = x` ¬± Z(Œ±/2) * (‚ÄãœÉ‚Äã‚Äã/sqrt(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86f918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "mean = df['limit_bal'].mean()\n",
    "n = len(df)\n",
    "std = np.std(df['limit_bal'], ddof=1)\n",
    "alpha = 0.05\n",
    "z = st.norm.ppf(1 - alpha/2)\n",
    "margin_of_error = z * (std / np.sqrt(n))\n",
    "ci_lower, ci_upper = mean - margin_of_error, mean + margin_of_error\n",
    "print(f\"95% Confidence Interval for limit_bal: [{ci_lower:.2f}, {ci_upper:.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdd84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = st.t.interval(\n",
    "    confidence=0.95,  # CI\n",
    "    df=len(df['limit_bal'])-1,  # margin error\n",
    "    loc=np.mean(df['limit_bal']), \n",
    "    scale=st.sem(df['limit_bal'])  # standard error of mean\n",
    ")\n",
    "\n",
    "print(f\"95% CI (using t-distribution): {ci}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acdcb5",
   "metadata": {},
   "source": [
    "## 7.2 üìä t-test<a id=\"ttest\"></a>\n",
    "\n",
    "### t-test: compare the value of mean (any statistic) of the default payment group vs non-default payment group to see if there is a statistically significant difference between the two groups.\n",
    "### if p-value < Œ± (0.05) then we reject the null hypothesis (the means are equal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45be00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# One-sample t-test\n",
    "stats.ttest_1samp(df['age'], popmean=35)\n",
    "\n",
    "# Independent two-sample t-test (e.g., limit_bal between default vs non-default)\n",
    "group1 = df[df['default_payment'] == 0]['limit_bal']\n",
    "group2 = df[df['default_payment'] == 1]['limit_bal']\n",
    "\n",
    "print('Group 1 (have not defaulted):',group1.mean())\n",
    "print('Group 2 (have defaulted):',group2.mean())\n",
    "print(stats.ttest_ind(group1, group2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1786d4ba",
   "metadata": {},
   "source": [
    "**The p-value is very less than the significance level of 0.05, so We reject the null hypothesis (the means are equal).**\n",
    "\n",
    "**We can conclude that there is a statistically significant difference between the means of the two groups.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa4010b",
   "metadata": {},
   "source": [
    "## 7.3 üßÆ Chi-Square Test<a id=\"chi-square\"></a>\n",
    "### Chi-Square Test: used to determine if there is a significant association between two categorical variables.\n",
    "### example: is there is a relationship between default payment and marital status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d1985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "categorical_features = [\"sex\", \"marriage\", \"education\", \"avg_delay\"]  \n",
    "\n",
    "results = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    contingency_table = pd.crosstab(df[col], df[\"default_payment\"])  # observed table\n",
    "    chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "    \n",
    "    results[col] = {\n",
    "        \"chi2_statistic\": chi2,\n",
    "        \"p_value\": p,\n",
    "        \"degrees_of_freedom\": dof,\n",
    "        \"significant\": p < 0.05\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be3513",
   "metadata": {},
   "source": [
    "**there is significant relationship between education and avg_delay with default_payment (the highest two chi2 values)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e9383e",
   "metadata": {},
   "source": [
    "## 8.1 ‚öñÔ∏è SMOTE Oversampling<a id=\"smote\"></a>\n",
    "since the target column is imbalanced, we can use SMOTE to generate synthetic samples for the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53482015",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['default_payment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3934f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"default_payment\"])\n",
    "y = df[\"default_payment\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d426b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "print(\"After SMOTE:\", Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c723c57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling using RobustScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "scaler = RobustScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc3a2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the scaler at models directory\n",
    "import joblib\n",
    "joblib.dump(scaler, 'models/robust_scaler.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652afe4f",
   "metadata": {},
   "source": [
    "## 9.1 ü§ñ Logistic Regression & ROC AUC<a id=\"logistic-baseline\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72313cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c58683",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "J = tpr - fpr\n",
    "best_idx = np.argmax(J)\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(\"AUC:\", roc_auc)\n",
    "print(\"Best threshold:\", best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38344a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "plt.plot([0,1], [0,1], 'k--')\n",
    "plt.scatter(fpr[best_idx], tpr[best_idx], marker='o', color='red', label=f'Best Threshold = {best_threshold:.2f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fab097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "y_pred_best = (y_probs >= best_threshold).astype(int)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_best))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_best))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_best))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4a181",
   "metadata": {},
   "source": [
    "## 9.2üìä Tree and Gradient Boosting Models<a id=\"tree-boosting\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6535df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_model.fit(X_resampled, y_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696aa144",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "best_threshold_rf = 0.5 \n",
    "y_pred_rf = (y_probs_rf >= best_threshold_rf).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de06ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"Confusion Matrix:\\n\", cm_rf)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_rf))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_rf))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_rf))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_probs_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb_model = XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5999f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob_xg = xgb_model.predict_proba(X_test)[:, 1]\n",
    "best_threshold_xg = 0.5\n",
    "y_pred_xg= (y_probs_rf >= best_threshold_xg).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_xg= confusion_matrix(y_test, y_pred_xg)\n",
    "print(\"Confusion Matrix:\\n\", cm_xg)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xg))\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xg))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_xg))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_xg))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_xg))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_prob_xg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8ce8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm_xg = confusion_matrix(y_test, y_pred_xg)\n",
    "disp_xg = ConfusionMatrixDisplay(confusion_matrix=cm_xg, display_labels=xgb_model.classes_)\n",
    "disp_xg.plot(cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7e5a0",
   "metadata": {},
   "source": [
    "## 9.3 üéØ Best Model<a id=\"best-model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac60b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(xgb_model, 'models/xgboost.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d221023",
   "metadata": {},
   "source": [
    "## 10.1 üîÑ Bootstrap Sampling<a id=\"bootstrap\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91605b22",
   "metadata": {},
   "source": [
    "### bootstrap sampling to see the CI for each metric to the model if it fitted with a population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c61623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample, shuffle\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "n_bootstrap = 1000\n",
    "bootstrap_results = {m: [] for m in metrics}\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    indices = np.random.choice(len(y_test), len(y_test), replace=True)\n",
    "    y_true_bs = y_test.iloc[indices]\n",
    "    y_pred_bs = y_pred_xg[indices]\n",
    "    y_prob_bs = y_prob_xg[indices]\n",
    "    \n",
    "    bootstrap_results['accuracy'].append(accuracy_score(y_true_bs, y_pred_bs))\n",
    "    bootstrap_results['precision'].append(precision_score(y_true_bs, y_pred_bs))\n",
    "    bootstrap_results['recall'].append(recall_score(y_true_bs, y_pred_bs))\n",
    "    bootstrap_results['f1'].append(f1_score(y_true_bs, y_pred_bs))\n",
    "    bootstrap_results['roc_auc'].append(roc_auc_score(y_true_bs, y_prob_bs))\n",
    "\n",
    "# ÿ≠ÿ≥ÿßÿ® 95% Confidence Interval\n",
    "for m in metrics:\n",
    "    lower = np.percentile(bootstrap_results[m], 2.5)\n",
    "    upper = np.percentile(bootstrap_results[m], 97.5)\n",
    "    print(f\"95% CI for {m}: [{lower:.4f}, {upper:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5659dd",
   "metadata": {},
   "source": [
    "## 10.2 üîÄ Permutation Testing<a id=\"permutation\"></a>\n",
    "### Permutation Test is to shuffle the labels and see if the model can still predict well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fe2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_auc = roc_auc_score(y_test, y_prob_xg)\n",
    "n_permutations = 1000\n",
    "count = 0\n",
    "\n",
    "for _ in range(n_permutations):\n",
    "    y_permuted = shuffle(y_test)\n",
    "    perm_auc = roc_auc_score(y_permuted, y_prob_xg)\n",
    "    if perm_auc >= observed_auc:\n",
    "        count += 1\n",
    "\n",
    "p_value = count / n_permutations\n",
    "print(f\"Permutation test p-value for ROC AUC: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040f799",
   "metadata": {},
   "source": [
    "**the model is not a coincidence and the prediction result is real since p-value > 0.05**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8845bf4",
   "metadata": {},
   "source": [
    "## 11.1 üåÄ KMeans & DBSCAN<a id=\"clustering\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148627d0",
   "metadata": {},
   "source": [
    " **first choice of us is to drop the outliers using IQR method since our data is not normally distributed, and then use K-mean clustering (this step is important because K mean is senstive to outliers)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc116d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['limit_bal', 'avg_delay', 'avg_utilization', 'avg_pay_ratio', 'R', 'F', 'M']\n",
    "X = df[features]\n",
    "X = remove_outliers_iqr(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7081cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "inertia = []\n",
    "K_range = range(2, 11)  \n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(K_range, inertia, 'bo-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83278af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_scores = []\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    score = silhouette_score(X, labels)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(K_range, silhouette_scores, 'ro-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ef643",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_k = 3\n",
    "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "X['cluster'] = kmeans_final.fit_predict(X)\n",
    "\n",
    "print(X['cluster'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15615c19",
   "metadata": {},
   "source": [
    "---\n",
    "**second choice is to continue with the original data without removing outliers and use DBSCAN because it is less sensitive to outliers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1914ae9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "features = [\n",
    "    'limit_bal', 'sex', 'education', 'marriage', 'age', \n",
    "    'status_sep', 'status_aug', 'status_jul', 'status_june', 'status_may', 'status_apr',\n",
    "    'avg_delay', 'max_delay', 'debt_sep', 'debt_aug', 'debt_jul', 'debt_june', 'debt_may', 'debt_apr',\n",
    "    'pay_sep', 'pay_aug', 'pay_jul', 'pay_june', 'pay_may', 'pay_apr',\n",
    "    'utilization_apr', 'utilization_may', 'utilization_june', 'utilization_jul', 'utilization_aug', 'utilization_sep',\n",
    "    'avg_utilization', 'pay_ratio_apr', 'pay_ratio_may', 'pay_ratio_june', 'pay_ratio_jul', 'pay_ratio_aug', 'pay_ratio_sep',\n",
    "    'avg_pay_ratio', 'pay_ratio_std', 'R', 'F', 'M'\n",
    "]\n",
    "\n",
    "X = df[features].values\n",
    "scaler = RobustScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1acd1a",
   "metadata": {},
   "source": [
    "## 11.2 üìâ PCA Transformation<a id=\"pca\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae34da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance_ratio.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689587c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Explained Variance vs Number of Components')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8758d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_optimal = (cumulative_variance >= 0.95).argmax() + 1\n",
    "print(\"Optimal number of components:\", n_components_optimal)\n",
    "\n",
    "pca_optimal = PCA(n_components=n_components_optimal)\n",
    "X_pca = pca_optimal.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11338e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51312d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.5, min_samples=500) \n",
    "clusters = dbscan.fit_predict(X_pca)\n",
    "\n",
    "print(\"Cluster distribution:\", Counter(clusters))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
